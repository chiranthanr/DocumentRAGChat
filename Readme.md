# Chat with PDF document using locally stored LLM via Ollama and Langchain

### This application uses a simple chat interface to query a locally running LLM through ollama and langchain.

#### To use this application follow these steps:

* Clone this repository to your local folder using '> git clone' *<repo name>*
* Install python 3.11 in your local system
* Install python 3.11 or later in your local system
* Install Ollama in your local computer with instructions in this link - https://github.com/ollama/ollama/blob/main/README.md
* Install all required libraries in your environment using '> pip install -r requirements.txt'
* Run Ollama in your local machine with the command '> ollama serve'.If the service is already running then you will 
  be notified accordingly
* Run the RAG Q&A application using '> chainlit run app.py' 
#### This will now open a chat interface where you can write your python related queries and get instant responses.